{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div width=50% style=\"display: block; margin: auto\">\n",
    "    <img src=\"figures/ucl-logo.svg\" width=100%>\n",
    "</div>\n",
    "\n",
    "\n",
    "### [UCL-ELEC0136 Data Acquisition and Processing Systems 2024]()\n",
    "University College London\n",
    "# Lab 4: Advanced Data Storage - Vector Databases and LLMs\n",
    "\n",
    "\n",
    "<hr width=70% style=\"float: left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT:** The content of this Notebook will not be evaluated in the final exam. The goal is to provide you with practical experience with the very trendy subject that LLMs are, for you to know how to use them, and provide you with enough to start your own LLM projects. \n",
    "\n",
    "This lab also serves as a good illustration of what this module is about: how data acquisition, storage, and processing all come together to create inteligent AI-driven applications.\n",
    "\n",
    "### Objectives\n",
    "* Perform CRUD operations on a Pinecone Vector Database.\n",
    "* Use Langchain to make queries to API accessed pre-trained LLM models (from Hugging Face and OpenAI).\n",
    "* Compare the performances of two pre-trained LLMs.\n",
    "* Use a Pinecone vector Database to perform Retrieval Augmentation of a pre-trained LLM, allowing it to both expend his knowledge base, and cite sources.\n",
    "\n",
    "### Outline\n",
    "\n",
    "This notebook has 3 parts:\n",
    "\n",
    "0. [Setting up](#0.-Setting-up)\n",
    "1. [CRUD operations on a vector database](#1-crud-operations-on-a-pinecone-vector-database)\n",
    "2. [Intro to LLMs and LangChain](#2.-Intro-to-LLMs-and-Langchain)\n",
    "3. [Retrieval Augmentation of a LLM using a vector database](#3.-Retrieval-Augmentation-of-a-LLM-using-a-vector-database)\n",
    "\n",
    "<hr width=70% style=\"float: left\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Setting up"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- The assignment repository contains a `requirements.txt` file, make sure to install all the librairies with the correct versions listed in this file in your daps conda environment.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.1 Create a Pinecone account and connect to your free-tier online vector database\n",
    "\n",
    "[Pinecone](https://www.pinecone.io) is a vector database service that helps developers build and deploy applications with high-performance similarity search and recommendation capabilities. It enables efficient storage and retrieval of vector data, making it easier to create personalized experiences and content recommendations in various applications, such as stable diffusion, LLMs chatbox, and many other AI applications.\n",
    "\n",
    "In this Notebook, we will use Pinecone's free tier to create and connect to a vector database, perform CRUD operations, and then use that database to power a LLM application.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "    \n",
    "- Follow the instructions in `pinecone_tutorial.pdf` to create a Pinecone account and get an API key for the free tier vector database. MAKE SURE TO KEEP A COPY OF YOUR API KEY.\n",
    "- Run the cell bellow to connect to your online vector database.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell (it may take a few seconds)\n",
    "import pinecone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   change PINECONE_API_KEY with your pinecone API key and run the cell\n",
    "#\n",
    "###########################\n",
    "\n",
    "PINECONE_API_KEY = \"210bffc8-1c2f-43b8-a7ee-5e513bdbb2c0\" #<--- TODO: your API key here \n",
    "\n",
    "pinecone.init(api_key=PINECONE_API_KEY, environment=\"gcp-starter\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "    \n",
    "- Use the [`list_indexes`](https://docs.pinecone.io/reference/list_indexes) function to return the list of Pinecone indexes you have on your database (it should be empty).\n",
    "- If it isn't empty, use the [`delete_index`](https://docs.pinecone.io/reference/delete_index) function to delete any indexes you may have on your database.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['langchain-retrieval-augmentation-fast']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   Check that your pinecone database does not contain any indexes, and delete them if there are any.\n",
    "#\n",
    "###########################\n",
    "\n",
    "# TODO : your code bellow\n",
    "pinecone.list_indexes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.2 Create a HuggingFace account and generate a free API key\n",
    "\n",
    "**Note:** you do not need this step to do part. [# 1. CRUD opperations on a Vector Database](#1.-CRUD-opperations-on-a-Vector-Database).\n",
    "\n",
    "\n",
    "[Hugging Face ü§ó](https://huggingface.co) is a company and open-source platform that specializes in natural language processing (NLP) and provides tools, libraries, and pre-trained models for building and deploying NLP applications. Their most well-known product is the Transformers library, which offers access to a wide range of pre-trained NLP models, making it easier for developers to work with text-based tasks such as language translation, sentiment analysis, and more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "    \n",
    "- Follow the instructions in `huggingface_tutorial.pdf` to create a HuggingFace account and get an API key for the free tier vector database. MAKE SURE TO KEEP A COPY OF YOUR API KEY.\n",
    "- Run the cell bellow to set an environment variable to your API key. `langchain.HuggingFaceHub` will use this environment variable when sending a request to the Hugging Face server to authentificate the connection.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "###########################\n",
    "# Task: \n",
    "#   change HUGGING_FACE_API_KEY with your Hugging Face API key and run the cell to set the environment variable HUGGINGFACEHUB_API_TOKEN\n",
    "#\n",
    "###########################\n",
    "\n",
    "HUGGING_FACE_API_KEY = \"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\" #<--- TODO: your Hugging Face API key here \n",
    "\n",
    "os.environ['HUGGINGFACEHUB_API_TOKEN'] = HUGGING_FACE_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.3 Create an OpenAI account and generate a free API key\n",
    "\n",
    "**Note:** you do not need this step to do part. [1. CRUD opperations on a Vector Database](#1.-crud-operations-on-a-pinecone-vector-database) and part. [2.1 Hugging Face LLM](#211-initializing-the-llm).\n",
    "\n",
    "[OpenAI](https://openai.com) is an artificial intelligence (AI) research laboratory consisting of the for-profit OpenAI LP and its non-profit parent company, OpenAI Inc.\n",
    "\n",
    "OpenAI provides an [API](https://platform.openai.com/docs/overview) that allows developers to access and integrate the capabilities of OpenAI's language models into their own applications, products, or services. The OpenAI API is based on models like GPT and allows developers to make use of powerful natural language processing (NLP) functionalities.\n",
    "\n",
    "**Unlike Hugging Face, OpenAI is not open source, and the free tier of their API is only available for 3 months after the creation of a new account, and has a lot of restriction (for exemple, you are limited to 3 requests per minutes for each model). However, their models are state of the art and very powerful, which is why we will use them in this lab.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "    \n",
    "- Follow the instructions in `openai_tutorial.pdf` to create a HuggingFace account and get an API key for the free tier vector database. MAKE SURE TO KEEP A COPY OF YOUR API KEY.\n",
    "- Run the cell bellow to set an environment variable to your API key. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   change OPENAI_API_KEY with your OpenAI API key and run the cell to set the environment variable HUGGINGFACEHUB_API_TOKEN\n",
    "#\n",
    "###########################\n",
    "\n",
    "\n",
    "OPENAI_API_KEY = \"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\" #<--- TODO: your OpenAI API key here \n",
    "\n",
    "os.environ['OPENAI_API_KEY'] = OPENAI_API_KEY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. CRUD operations on a Pinecone Vector Database\n",
    "\n",
    "*Source: https://docs.pinecone.io/docs/quickstart*\n",
    "\n",
    "\n",
    "In this part, we will familiarize ourselve with basic operations on a Pinecode Vector Database."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>üë©‚Äçüíªüë®‚Äçüíª Optional action</b>\n",
    "\n",
    "- Very short video intro on Vector Databases: [Vector databases are so hot right now. What are they?](https://www.youtube.com/watch?v=klTvEwg3oJ4)\n",
    "- Short read: [Everything you need to know about Pinecone ‚Äì A Vector Database](https://www.packtpub.com/article-hub/everything-you-need-to-know-about-pinecone-a-vector-database).\n",
    "- In depth read: [What is a Vector Database & How Does it Work? Use Cases + Examples](https://www.pinecone.io/learn/vector-database/).\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-heading alert-danger\" style=\"background-color: white; border: 2px solid; border-radius: 5px; color: #000; border-color:#AAA; padding: 10px\">\n",
    "    <b>üíé Tip</b>\n",
    "\n",
    "Once your connection to your Pinecone database is established (which should have been done in part. 0.1), you do not need to use `pinecone.init` anymore and can directly use the API requests.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 CRUD operations on a Vector Database - Create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "    \n",
    "- Use the [`create_index`](https://docs.pinecone.io/reference/create_index) function to create an index, name it `quickstart`, set the dimension to 8, and use the metric `euclidean`.\n",
    "- Check that your database contains an index called `quickstart`.\n",
    "- Use the [`describe_index`](https://docs.pinecone.io/reference/describe_index) function to get informations about the index `quickstart`.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pinecone' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\zjy\\github-classroom\\UCL-ELEC0136\\4-advanced-data-storage-LagrKevin6\\4-Advanced-Data-Storage.ipynb Cell 22\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zjy/github-classroom/UCL-ELEC0136/4-advanced-data-storage-LagrKevin6/4-Advanced-Data-Storage.ipynb#X30sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m###########################\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zjy/github-classroom/UCL-ELEC0136/4-advanced-data-storage-LagrKevin6/4-Advanced-Data-Storage.ipynb#X30sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# Task: \u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zjy/github-classroom/UCL-ELEC0136/4-advanced-data-storage-LagrKevin6/4-Advanced-Data-Storage.ipynb#X30sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39m#   Create an index called quickstart, check that it has been added to your Pinecone Vector DB, and get information about that index\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zjy/github-classroom/UCL-ELEC0136/4-advanced-data-storage-LagrKevin6/4-Advanced-Data-Storage.ipynb#X30sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# TODO : your code bellow\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zjy/github-classroom/UCL-ELEC0136/4-advanced-data-storage-LagrKevin6/4-Advanced-Data-Storage.ipynb#X30sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m#DAPSweek4 = _get_api_instance()\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/zjy/github-classroom/UCL-ELEC0136/4-advanced-data-storage-LagrKevin6/4-Advanced-Data-Storage.ipynb#X30sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m pinecone\u001b[39m.\u001b[39mcreate_index(name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mquickstart\u001b[39m\u001b[39m'\u001b[39m,dimension \u001b[39m=\u001b[39m \u001b[39m8\u001b[39m, metric\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39meuclidean\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zjy/github-classroom/UCL-ELEC0136/4-advanced-data-storage-LagrKevin6/4-Advanced-Data-Storage.ipynb#X30sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m pinecone\u001b[39m.\u001b[39mlist_indexes()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zjy/github-classroom/UCL-ELEC0136/4-advanced-data-storage-LagrKevin6/4-Advanced-Data-Storage.ipynb#X30sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m pinecone\u001b[39m.\u001b[39mdescribe_index(name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mquickstart\u001b[39m\u001b[39m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'pinecone' is not defined"
     ]
    }
   ],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   Create an index called quickstart, check that it has been added to your Pinecone Vector DB, and get information about that index\n",
    "#\n",
    "###########################\n",
    "\n",
    "# TODO : your code bellow\n",
    "#DAPSweek4 = _get_api_instance()\n",
    "pinecone.create_index(name='quickstart',dimension = 8, metric='euclidean')\n",
    "\n",
    "pinecone.list_indexes()\n",
    "\n",
    "pinecone.describe_index(name='quickstart')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "    \n",
    "- Use the [`Index`](https://docs.pinecone.io/docs/python-client#index) class to contruct an `Index` object from the index `quickstart`. \n",
    "- Use the [`upsert`](https://docs.pinecone.io/docs/python-client#indexupsert) method to push the 5 vectors in the cell bellow to the index `quickstart`.\n",
    "- Use the [`describe_index_stats`](https://docs.pinecone.io/docs/python-client#indexdescribe_index_stats) method to get statistics about the index's contents.\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-heading alert-danger\" style=\"background-color: white; border: 2px solid; border-radius: 5px; color: #000; border-color:#AAA; padding: 10px\">\n",
    "    <b>üíé Tip</b>\n",
    "\n",
    "Wait for a few seconds after you use `upsert` before querying the index with `describe_index_stats` as the data needs a bit of time to be saved.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   Construct an Index object and use it to push the 5 vectors in data to your index, and get statistics about the index\n",
    "#\n",
    "###########################\n",
    "index_me = pinecone.Index(\"quickstart\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 CRUD operations on a Vector Database - Read"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "    \n",
    "- Use the [`query`](https://docs.pinecone.io/docs/python-client#indexquery) method to search for the 3 closest vectors to the `target_vector` in the index `quickstart`.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'matches': [{'id': 'C', 'score': 0.0, 'values': []},\n",
       "             {'id': 'D', 'score': 0.0799999237, 'values': []},\n",
       "             {'id': 'B', 'score': 0.0800000429, 'values': []}],\n",
       " 'namespace': ''}"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   find the 3 nearest vectors to the target_vector\n",
    "#\n",
    "###########################\n",
    "target_vector = [0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3]\n",
    "\n",
    "index_me.query(target_vector,top_k= 3)\n",
    "# TODO : your code bellow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "    \n",
    "- Use the [`query`](https://docs.pinecone.io/docs/python-client#indexquery) method to search for the 3 closest vectors to the `target_vector` the index `quickstart` that are dramas.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'matches': [{'id': 'D', 'score': 0.0799999237, 'values': []},\n",
       "             {'id': 'E', 'score': 0.319999695, 'values': []}],\n",
       " 'namespace': ''}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   find the 3 nearest vectors to the target_vector that are dramas\n",
    "#\n",
    "###########################\n",
    "\n",
    "index_me.query(target_vector,top_k= 3,filter={\n",
    "        'genre': {'$in': ['drama']}\n",
    "        })\n",
    "# TODO : your code bellow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 CRUD operations on a Vector Database - Update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want to change the values associated to vectors A and D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'matches': [{'id': 'A',\n",
      "              'metadata': {'genre': 'comedy', 'year': 2020.0},\n",
      "              'score': 0.0,\n",
      "              'values': [0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1]}],\n",
      " 'namespace': ''}\n",
      "{'matches': [{'id': 'D',\n",
      "              'metadata': {'genre': 'drama'},\n",
      "              'score': 0.0,\n",
      "              'values': [0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4]}],\n",
      " 'namespace': ''}\n"
     ]
    }
   ],
   "source": [
    "# Run this cell\n",
    "print(index_me.query(\n",
    "  id = \"A\",\n",
    "  top_k = 1,\n",
    "  include_values = True,\n",
    "    include_metadata = True\n",
    "))\n",
    "\n",
    "print(index_me.query(\n",
    "  id = \"D\",\n",
    "  top_k = 1,\n",
    "  include_values = True,\n",
    "  include_metadata = True\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "    \n",
    "- Use the [`update`](https://docs.pinecone.io/docs/python-client#indexupdate) method to do the following updates:\n",
    "    - Replace all the values of `A` by `0.6`, change the genre to `action-comedy`.\n",
    "    - Replace all the values of `D` by `-0.4`, add a `year` field in the metadata and set it to `2019`.\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-heading alert-danger\" style=\"background-color: white; border: 2px solid; border-radius: 5px; color: #000; border-color:#AAA; padding: 10px\">\n",
    "    <b>üíé Tip</b>\n",
    "\n",
    "Wait for a few seconds after you use `update` before querying the index as the data needs a bit of time to be saved.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "update_size = index_me.describe_index_stats().dimension\n",
    "update_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   Update vectors A and D as indicated above.\n",
    "#\n",
    "###########################\n",
    "\n",
    "\n",
    "# TODO : your code bellow\n",
    "#A_all = index_me.fetch(ids=['A'])\n",
    "#A_all['values']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.6 0.6 0.6 0.6 0.6 0.6 0.6 0.6]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "A_update = 0.6*np.ones(update_size)\n",
    "D_update = -0.4*np.ones(update_size)\n",
    "print(A_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(A_update)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'index_me' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\zjy\\github-classroom\\UCL-ELEC0136\\4-advanced-data-storage-LagrKevin6\\4-Advanced-Data-Storage.ipynb Cell 38\u001b[0m line \u001b[0;36m2\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zjy/github-classroom/UCL-ELEC0136/4-advanced-data-storage-LagrKevin6/4-Advanced-Data-Storage.ipynb#X52sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtime\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/zjy/github-classroom/UCL-ELEC0136/4-advanced-data-storage-LagrKevin6/4-Advanced-Data-Storage.ipynb#X52sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m update_response \u001b[39m=\u001b[39m index_me\u001b[39m.\u001b[39mupdate(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zjy/github-classroom/UCL-ELEC0136/4-advanced-data-storage-LagrKevin6/4-Advanced-Data-Storage.ipynb#X52sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m     \u001b[39mid\u001b[39m\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mA\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zjy/github-classroom/UCL-ELEC0136/4-advanced-data-storage-LagrKevin6/4-Advanced-Data-Storage.ipynb#X52sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     values \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(A_update),\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zjy/github-classroom/UCL-ELEC0136/4-advanced-data-storage-LagrKevin6/4-Advanced-Data-Storage.ipynb#X52sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     set_metadata\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39mgenre\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m'\u001b[39m\u001b[39maction-comedy\u001b[39m\u001b[39m'\u001b[39m}\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zjy/github-classroom/UCL-ELEC0136/4-advanced-data-storage-LagrKevin6/4-Advanced-Data-Storage.ipynb#X52sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m )\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zjy/github-classroom/UCL-ELEC0136/4-advanced-data-storage-LagrKevin6/4-Advanced-Data-Storage.ipynb#X52sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m update_response \u001b[39m=\u001b[39m index_me\u001b[39m.\u001b[39mupdate(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zjy/github-classroom/UCL-ELEC0136/4-advanced-data-storage-LagrKevin6/4-Advanced-Data-Storage.ipynb#X52sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     \u001b[39mid\u001b[39m\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mD\u001b[39m\u001b[39m'\u001b[39m,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zjy/github-classroom/UCL-ELEC0136/4-advanced-data-storage-LagrKevin6/4-Advanced-Data-Storage.ipynb#X52sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     values \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(D_update),\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zjy/github-classroom/UCL-ELEC0136/4-advanced-data-storage-LagrKevin6/4-Advanced-Data-Storage.ipynb#X52sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m     set_metadata\u001b[39m=\u001b[39m{\u001b[39m'\u001b[39m\u001b[39myear\u001b[39m\u001b[39m'\u001b[39m: \u001b[39m2019\u001b[39m}\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zjy/github-classroom/UCL-ELEC0136/4-advanced-data-storage-LagrKevin6/4-Advanced-Data-Storage.ipynb#X52sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m )\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zjy/github-classroom/UCL-ELEC0136/4-advanced-data-storage-LagrKevin6/4-Advanced-Data-Storage.ipynb#X52sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m time\u001b[39m.\u001b[39msleep(\u001b[39m5\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'index_me' is not defined"
     ]
    }
   ],
   "source": [
    "import time\n",
    "update_response = index_me.update(\n",
    "    id='A',\n",
    "    values = list(A_update),\n",
    "    set_metadata={'genre': 'action-comedy'}\n",
    ")\n",
    "\n",
    "update_response = index_me.update(\n",
    "    id='D',\n",
    "    values = list(D_update),\n",
    "    set_metadata={'year': 2019}\n",
    ")\n",
    "\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'matches': [{'id': 'A',\n",
      "              'metadata': {'genre': 'action-comedy', 'year': 2020.0},\n",
      "              'score': 0.0,\n",
      "              'values': [0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6]}],\n",
      " 'namespace': ''}\n",
      "{'matches': [{'id': 'D',\n",
      "              'metadata': {'genre': 'drama', 'year': 2019.0},\n",
      "              'score': 0.0,\n",
      "              'values': [-0.4, -0.4, -0.4, -0.4, -0.4, -0.4, -0.4, -0.4]}],\n",
      " 'namespace': ''}\n"
     ]
    }
   ],
   "source": [
    "# Run this cell\n",
    "print(index_me.query(\n",
    "  id = \"A\",\n",
    "  top_k = 1,\n",
    "  include_values = True,\n",
    "  include_metadata = True\n",
    "))\n",
    "\n",
    "print(index_me.query(\n",
    "  id = \"D\",\n",
    "  top_k = 1,\n",
    "  include_values = True,\n",
    "  include_metadata = True\n",
    "))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 CRUD operations on a Vector Database - Delete"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "    \n",
    "- Use the [`delete`](https://docs.pinecone.io/docs/python-client#indexdelete) method to delete the vectors `B` and `C`.\n",
    "- Check with `describe_index_stats` that your index now contains 3 vectors\n",
    "\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-heading alert-danger\" style=\"background-color: white; border: 2px solid; border-radius: 5px; color: #000; border-color:#AAA; padding: 10px\">\n",
    "    <b>üíé Tip</b>\n",
    "\n",
    "Wait for a few seconds after you use `upsert` before querying the index with `describe_index_stats` as the data needs a bit of time to be saved.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   Delete vectors B and C from the index.\n",
    "#\n",
    "###########################\n",
    "\n",
    "\n",
    "# TODO : your code bellow\n",
    "delete_response = index_me.delete(ids= ['B','D'])\n",
    "\n",
    "time.sleep(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'dimension': 8,\n",
       " 'index_fullness': 3e-05,\n",
       " 'namespaces': {'': {'vector_count': 3}},\n",
       " 'total_vector_count': 3}"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index_me.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "    \n",
    "- Use the [`delete_index`](https://docs.pinecone.io/reference/delete_index) function to delete the index `quickstart`.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   delete the index quickstart from your Pinecode Vector Database\n",
    "#\n",
    "###########################\n",
    "\n",
    "# TODO : your code bellow\n",
    "pinecone.delete_index('quickstart')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "pinecone.list_indexes()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Intro to LLMs and LangChain\n",
    "\n",
    "**Context:**\n",
    "\n",
    "* **Large language models (LLMs)** are a class of artificial intelligence models that have been trained on vast amounts of text data to understand and generate human-like text. These models are based on deep learning techniques, such as neural networks, and have many parameters, often numbering in the hundreds of millions or even billions. Some well-known examples include OpenAI's GPT-3 and GPT-4, and Google's BERT and T5 models.\n",
    "\n",
    "* \" [**LangChain**](https://python.langchain.com/docs/get_started/introduction) is an open source framework that lets software developers working with artificial intelligence (AI) and its machine learning subset combine large language models with other external components to develop LLM-powered applications. The goal of LangChain is to link powerful LLMs, such as OpenAI's GPT-3.5 and GPT-4, to an array of external data sources to create and reap the benefits of natural language processing (NLP) applications. \" - [Source](https://www.techtarget.com/searchenterpriseai/definition/LangChain#:~:text=LangChain%20is%20an%20open%20source,to%20develop%20LLM%2Dpowered%20applications.)\n",
    "\n",
    "\n",
    "\n",
    "**Although we are doing these operations in a Jupyter Notebook, the exact same code can be used to program server-hosted web applications that could perform real-world tasks.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>üë©‚Äçüíªüë®‚Äçüíª Optional action</b>\n",
    "\n",
    "- In depth read: [LangChain AI Handbook](https://www.pinecone.io/learn/series/langchain/).\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "In this part, we will use LangChain to deploy two pre-trained LLMs, one from Hugging Face Hub, and one from OpenAI's API.\n",
    "\n",
    "*Source: [https://www.pinecone.io/learn/series/langchain/langchain-intro/](https://www.pinecone.io/learn/series/langchain/langchain-intro/)*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Hugging Face LLM\n",
    "\n",
    "Let's use a pre-trained Google language model, [flan-t5-xxl](https://huggingface.co/google/flan-t5-xxl), hosted on the Hugging Face Hub, that we will access for free through Hugging Face's API. More specifically, we will use the HuggingFaceHub module of the langchain library, which will query Hugging Face's API for us.\n",
    "\n",
    "### 2.1.1 Initializing the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Run the cell below to initialize the connection to the LLM.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Anaconda\\envs\\daps\\Lib\\site-packages\\huggingface_hub\\utils\\_deprecation.py:127: FutureWarning: '__init__' (from 'huggingface_hub.inference_api') is deprecated and will be removed from version '0.19.0'. `InferenceApi` client is deprecated in favor of the more feature-complete `InferenceClient`. Check out this guide to learn how to convert your script to use it: https://huggingface.co/docs/huggingface_hub/guides/inference#legacy-inferenceapi-client.\n",
      "  warnings.warn(warning_message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Run this cell\n",
    "\n",
    "from langchain import HuggingFaceHub\n",
    "\n",
    "# initialize Hub LLM\n",
    "hub_llm = HuggingFaceHub(\n",
    "        repo_id='google/flan-t5-xxl',\n",
    "    model_kwargs={'temperature':1e-2} #Best temperature found: 1e-2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1.2 Use LangChain to ask a question to the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do to query a LLM is to create a **prompt template**. A prompt template contains instructions to generate a prompt in a reproductible way. It contains a text string (\"the template\"), that can take in a set of parameters from the end user and generates a prompt (the input variables).\n",
    "\n",
    "For example: \n",
    "\n",
    "* We want a LLM model to tell what language a sentence is written in, then an appropriate prompt template would be: \n",
    "    * `\"What language is the sentense \"{sentence}\" written in?\"`\n",
    "    * Here, `sentence` is the only input variable. \n",
    "\n",
    "<br/>\n",
    "\n",
    "* We want a LLM model to generate jokes about a topic, while specifying what type of jokes, then an appropriate prompt template would be: \n",
    "    * `\"Make a {type} of joke about {subject}\"\"`\n",
    "    * Here, `type` and `subject` are the input variables.\n",
    "\n",
    "<br/>\n",
    "\n",
    "\n",
    "**Note:** `google/flan-t5-xxl` is a small LLM, best suited to give short answers. Although it is enough for us to explore LLMs today, for a real application bigger models would be better suited.\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Using [langchain.PromptTemplate](https://api.python.langchain.com/en/latest/prompts/langchain.prompts.prompt.PromptTemplate.html), create a prompt template with an input variable called `name` that asks the language model to give the date of birth of a historical figure. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   Create a PromptTemplate with an input variable called `name` that asks the language model to give the date of birth of a historical figure.\n",
    "#\n",
    "###########################\n",
    "\n",
    "from langchain import PromptTemplate\n",
    "\n",
    "# TODO : your code bellow\n",
    "\n",
    "# Instantiation using from_template (recommended)\n",
    "prompt_me = PromptTemplate.from_template(\"Please find the Date of Birth of {name}\")\n",
    "#prompt_me.format(name=\"me\")\n",
    "\n",
    "# Instantiation using initializer\n",
    "#prompt = PromptTemplate(input_variables=[\"foo\"], template=\"Say {foo}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Using langchain.LLMChain, create a chain to run the prompt by our LLM.\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "<div class=\"alert alert-heading alert-danger\" style=\"background-color: white; border: 2px solid; border-radius: 5px; color: #000; border-color:#AAA; padding: 10px\">\n",
    "    <b>üíé Tip</b>\n",
    "\n",
    "An LLMChain consists of a PromptTemplate and a language model (either an LLM or chat model). It formats the prompt template using the input key values provided, passes the formatted string to LLM and returns the LLM output.\n",
    "\n",
    "**Example:** \n",
    "llm_chain = LLMChain(\n",
    "    prompt=your_PromptTemplate_template,\n",
    "    llm=your_llm\n",
    ")\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   Create a LLMchain with your prompt and your hub_llm\n",
    "#\n",
    "###########################\n",
    "\n",
    "from langchain import LLMChain\n",
    "\n",
    "# TODO : your code bellow\n",
    "llm_Ch = LLMChain(prompt=prompt_me,llm = hub_llm)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Use `LLMChain.run()` or `LLMChain.predict()` to ask for the date of birth of Napoleon.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1769-01-06'"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   Asks the language model to give the date of birth of a historical figure.\n",
    "#\n",
    "###########################\n",
    "\n",
    "# TODO : your code bellow\n",
    "llm_Ch.run('Napoleaon')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SPOILER:** Well, that's not amazing. The LLM understood we wanted a date, and we even got something close to Napoleon's real date of birth, but the results is wrong. This is because this LLM is small and not adapted to this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 OpenAI LLM\n",
    "\n",
    "In this part, we will task a more powerfull openAI LLM called [`text-davinci-003`](https://platform.openai.com/docs/models/gpt-3), a variant of GPT3,to perform the same task we instructed the Hugging Face hosted model, and see if we get better results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.1 Initializing the LLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Run the cell bellow to innitialize the connection to the LLM.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "\n",
    "from langchain.llms import OpenAI\n",
    "\n",
    "davinci = OpenAI(model_name='text-davinci-003', openai_api_key =  OPENAI_API_KEY)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2.2 Use LangChain to ask a question to the LLM\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Using langchain.LLMChain, create a chain to run the same prompt as in part. 2.1.2 by our OpenAI LLM.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   Create a LLMchain with your prompt and your hub_llm\n",
    "#\n",
    "###########################\n",
    "\n",
    "# TODO : your code bellow\n",
    "\n",
    "prompt_me = PromptTemplate.from_template(\"Please find the Date of Birth of {name}\") #same as 2.1.2\n",
    "\n",
    "llm_Ch2 = LLMChain(prompt=prompt_me,llm = davinci)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Use `LLMChain.run()` or `LLMChain.predict()` to ask for the date of birth of Napoleon.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nSu Shi, also known as Su Dongpo, was born 10 June 1037 in Meishan, Sichuan province, China.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   Asks the language model to give the date of birth of a historical figure.\n",
    "#\n",
    "###########################\n",
    "\n",
    "# TODO : your code bellow\n",
    "llm_Ch2.run('Napolean')\n",
    "llm_Ch2.predict(name = 'Su Shi')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**SPOILER:** Alright that's better, Napoleon was indeed born of August 15, 1769. Let's ask something more complex and recent and see how our LLM does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Using [langchain.PromptTemplate](https://api.python.langchain.com/en/latest/prompts/langchain.prompts.prompt.PromptTemplate.html), create a new prompt template with an input variable called `year` that asks the language model to give the winner of the FIFA world cup on a given year.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   Create a PromptTemplate with an input variable called `year` that asks the language model to give the winner of the FIFA world cup of that year.\n",
    "###########################\n",
    "\n",
    "# TODO : your code bellow\n",
    "prompt_me2 = PromptTemplate.from_template(\"Please find the Winner of the FIFA world cup of the year {year}\") #same as 2.1.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Using langchain.LLMChain, create a chain to run the new prompt template by our OpenAI LLM.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   Create a LLMchain with your prompt and your hub_llm\n",
    "#\n",
    "###########################\n",
    "\n",
    "llm_Ch3 = LLMChain(prompt=prompt_me2,llm = davinci)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Use `LLMChain.run()` or `LLMChain.predict()` to ask for the winner of the 2022 FIFA world cup.\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nThe FIFA World Cup of 2022 has not yet taken place, so no winner has been determined.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   Asks the language model to ask for the winner of the 2022 FIFA world cup\n",
    "#\n",
    "###########################\n",
    "\n",
    "# TODO : your code bellow\n",
    "llm_Ch3.run(2022)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The LLM doesn't know what happened in 2022 because it was trained before that year. How can we update the LLM's knowledge to make it up to date? With Retrieval augmentation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Retrieval Augmentation of a LLM using a Vector Database\n",
    "\n",
    "**WARNING:** Some of the cells in this part may take some time to run as we are working with a lot of data.\n",
    "\n",
    "The most powerful LLMs in the world have no idea about recent world events, nor can they cite sources. In general a LLM will only have knowledge about what it has been exposed to during training. For LLMs, the world exists as a static snapshot of the world as it was within their training data. \n",
    "\n",
    "A solution to this problem is **retrieval augmentation**: we retrieve relevant information from an external knowledge base and give that information to our LLM.\n",
    "\n",
    "*Source: https://docs.pinecone.io/docs/langchain#retrieval-augmentation-in-langchain*\n",
    "\n",
    "\n",
    "<div width=50% style=\"display: block; margin: auto\">\n",
    "    <img src=\"figures/augmentation.png\" width=70%>\n",
    "</div>\n",
    "\n",
    "To perform **retrieval augmentation**, we will embed data with an **embedding model**, store the embedded data into a **vector database index**, and create a **LangChain vectorstore** that will use the index and the embedding model to find relevant information to the prompt sent by the user, and feed the most relevant results found in the database to the **LLM** to provide it with context and sources."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Building a knowledge base with Vector Embedding \n",
    "\n",
    "Source: [Creating the knowledge base](https://www.pinecone.io/learn/series/langchain/langchain-retrieval-augmentation/#Creating-the-Knowledge-Base)\n",
    "\n",
    "This part concists in taking a dataset of relevant content we want to augment our LLM with (it could be code documentation for an LLM that needs to help write code, company documents for an internal chatbot...), process it, and embedded it into vectors. \n",
    "\n",
    "You can look [here](https://www.pinecone.io/learn/series/langchain/langchain-retrieval-augmentation/#Creating-the-Knowledge-Base) fore the full processs, which you would have to do if you wanted to augment your LLM for a specific use-case.\n",
    "\n",
    "**For the sake of simplicity, we will use a pre-embembeded dataset from pinecone_dataset and upload it to a Pinecone Vector database. Unfortunatly, this dataset doesn't contain data on the 2022 FIFA world cup, but the process used here can be applied with other datasets, giving you an idea of the process should you want to use it for personal projects.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Run the cell bellow to import and process the pre-embedded data we will use for retrieval augmentation\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting apache_beamNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Obtaining dependency information for apache_beam from https://files.pythonhosted.org/packages/43/c3/c35f521f79698d749c016179acf0718c6d9c8a6ecc9d2b45f84ff92c6be2/apache_beam-2.51.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached apache_beam-2.51.0-cp311-cp311-win_amd64.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: crcmod<2.0,>=1.7 in c:\\anaconda\\envs\\daps\\lib\\site-packages (from apache_beam) (1.7)\n",
      "Requirement already satisfied: orjson<4,>=3.9.7 in c:\\anaconda\\envs\\daps\\lib\\site-packages (from apache_beam) (3.9.10)\n",
      "Collecting dill<0.3.2,>=0.3.1.1 (from apache_beam)\n",
      "  Using cached dill-0.3.1.1-py3-none-any.whl\n",
      "Collecting cloudpickle~=2.2.1 (from apache_beam)\n",
      "  Using cached cloudpickle-2.2.1-py3-none-any.whl (25 kB)\n",
      "Collecting fastavro<2,>=0.23.6 (from apache_beam)\n",
      "  Obtaining dependency information for fastavro<2,>=0.23.6 from https://files.pythonhosted.org/packages/1c/d5/d8a47ac699fcdd83c2d54d095183a715170ce56162d67e451c6d61bf7565/fastavro-1.9.0-cp311-cp311-win_amd64.whl.metadata\n",
      "  Using cached fastavro-1.9.0-cp311-cp311-win_amd64.whl.metadata (5.7 kB)\n",
      "Collecting fasteners<1.0,>=0.3 (from apache_beam)\n",
      "  Obtaining dependency information for fasteners<1.0,>=0.3 from https://files.pythonhosted.org/packages/61/bf/fd60001b3abc5222d8eaa4a204cd8c0ae78e75adc688f33ce4bf25b7fafa/fasteners-0.19-py3-none-any.whl.metadata\n",
      "  Using cached fasteners-0.19-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: grpcio!=1.48.0,<2,>=1.33.1 in c:\\anaconda\\envs\\daps\\lib\\site-packages (from apache_beam) (1.59.2)\n",
      "Collecting hdfs<3.0.0,>=2.1.0 (from apache_beam)\n",
      "  Using cached hdfs-2.7.3-py3-none-any.whl\n",
      "Collecting httplib2<0.23.0,>=0.8 (from apache_beam)\n",
      "  Using cached httplib2-0.22.0-py3-none-any.whl (96 kB)\n",
      "Collecting js2py<1,>=0.74 (from apache_beam)\n",
      "  Using cached Js2Py-0.74-py3-none-any.whl (1.0 MB)\n",
      "Requirement already satisfied: numpy<1.25.0,>=1.14.3 in c:\\anaconda\\envs\\daps\\lib\\site-packages (from apache_beam) (1.24.4)\n",
      "Requirement already satisfied: objsize<0.7.0,>=0.6.1 in c:\\anaconda\\envs\\daps\\lib\\site-packages (from apache_beam) (0.6.1)\n",
      "Requirement already satisfied: packaging>=22.0 in c:\\anaconda\\envs\\daps\\lib\\site-packages (from apache_beam) (23.2)\n",
      "Requirement already satisfied: pymongo<5.0.0,>=3.8.0 in c:\\anaconda\\envs\\daps\\lib\\site-packages (from apache_beam) (3.11.0)\n",
      "Requirement already satisfied: proto-plus<2,>=1.7.1 in c:\\anaconda\\envs\\daps\\lib\\site-packages (from apache_beam) (1.22.3)\n",
      "Requirement already satisfied: protobuf!=4.0.*,!=4.21.*,!=4.22.0,!=4.23.*,!=4.24.0,!=4.24.1,!=4.24.2,<4.25.0,>=3.20.3 in c:\\anaconda\\envs\\daps\\lib\\site-packages (from apache_beam) (3.20.3)\n",
      "Requirement already satisfied: pydot<2,>=1.2.0 in c:\\anaconda\\envs\\daps\\lib\\site-packages (from apache_beam) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil<3,>=2.8.0 in c:\\anaconda\\envs\\daps\\lib\\site-packages (from apache_beam) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2018.3 in c:\\anaconda\\envs\\daps\\lib\\site-packages (from apache_beam) (2023.3.post1)\n",
      "Requirement already satisfied: regex>=2020.6.8 in c:\\anaconda\\envs\\daps\\lib\\site-packages (from apache_beam) (2023.10.3)\n",
      "Requirement already satisfied: requests<3.0.0,>=2.24.0 in c:\\anaconda\\envs\\daps\\lib\\site-packages (from apache_beam) (2.31.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.0 in c:\\anaconda\\envs\\daps\\lib\\site-packages (from apache_beam) (4.8.0)\n",
      "Requirement already satisfied: zstandard<1,>=0.18.0 in c:\\anaconda\\envs\\daps\\lib\\site-packages (from apache_beam) (0.22.0)\n",
      "Requirement already satisfied: pyarrow<12.0.0,>=3.0.0 in c:\\anaconda\\envs\\daps\\lib\\site-packages (from apache_beam) (11.0.0)\n",
      "Requirement already satisfied: docopt in c:\\anaconda\\envs\\daps\\lib\\site-packages (from hdfs<3.0.0,>=2.1.0->apache_beam) (0.6.2)\n",
      "Requirement already satisfied: six>=1.9.0 in c:\\anaconda\\envs\\daps\\lib\\site-packages (from hdfs<3.0.0,>=2.1.0->apache_beam) (1.16.0)\n",
      "Requirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in c:\\anaconda\\envs\\daps\\lib\\site-packages (from httplib2<0.23.0,>=0.8->apache_beam) (3.1.1)\n",
      "Requirement already satisfied: tzlocal>=1.2 in c:\\anaconda\\envs\\daps\\lib\\site-packages (from js2py<1,>=0.74->apache_beam) (5.2)\n",
      "Requirement already satisfied: pyjsparser>=2.5.1 in c:\\anaconda\\envs\\daps\\lib\\site-packages (from js2py<1,>=0.74->apache_beam) (2.7.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\anaconda\\envs\\daps\\lib\\site-packages (from requests<3.0.0,>=2.24.0->apache_beam) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\anaconda\\envs\\daps\\lib\\site-packages (from requests<3.0.0,>=2.24.0->apache_beam) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\anaconda\\envs\\daps\\lib\\site-packages (from requests<3.0.0,>=2.24.0->apache_beam) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\anaconda\\envs\\daps\\lib\\site-packages (from requests<3.0.0,>=2.24.0->apache_beam) (2023.7.22)\n",
      "Requirement already satisfied: tzdata in c:\\anaconda\\envs\\daps\\lib\\site-packages (from tzlocal>=1.2->js2py<1,>=0.74->apache_beam) (2023.3)\n",
      "Using cached apache_beam-2.51.0-cp311-cp311-win_amd64.whl (5.0 MB)\n",
      "Using cached fastavro-1.9.0-cp311-cp311-win_amd64.whl (497 kB)\n",
      "Using cached fasteners-0.19-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: httplib2, fasteners, fastavro, dill, cloudpickle, js2py, hdfs, apache_beam\n",
      "Successfully installed apache_beam-2.51.0 cloudpickle-2.2.1 dill-0.3.1.1 fastavro-1.9.0 fasteners-0.19 hdfs-2.7.3 httplib2-0.22.0 js2py-0.74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: The script fastavro.exe is installed in 'C:\\Users\\zjy\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "  WARNING: The scripts hdfscli-avro.exe and hdfscli.exe are installed in 'C:\\Users\\zjy\\AppData\\Roaming\\Python\\Python311\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "multiprocess 0.70.15 requires dill>=0.3.7, but you have dill 0.3.1.1 which is incompatible.\n"
     ]
    }
   ],
   "source": [
    "# Run this cell\n",
    "%pip install --user apache_beam\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'url', 'title', 'text'],\n",
       "    num_rows: 10000\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#import pinecone_datasets\n",
    "\n",
    "# We import a dataset\n",
    "#dataset = pinecone_datasets.load_dataset('wikipedia-simple-text-embedding-ada-002-100K')\n",
    "\n",
    "from datasets import load_dataset\n",
    "\n",
    "dataset =  load_dataset(\"wikipedia\", \"20220301.simple\", split='train[:10000]')\n",
    "\n",
    "# We drop sparse_values as they are not needed for this example\n",
    "#dataset.documents.drop(['metadata'], axis=1, inplace=True)\n",
    "#dataset.documents.rename(columns={'blob': 'metadata'}, inplace=True)\n",
    "# We will use rows of the dataset up to index 30_000 to make the upload to the Pinecone Vector Database faster\n",
    "#dataset.documents.drop(dataset.documents.index[30_000:], inplace=True)\n",
    "\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Run the cell bellow to check that your pinecone database does not contain any indexes, and delete them if there are any.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "pinecone.list_indexes()\n",
    "\n",
    "#pinecone.delete_index(\"index_name\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Use the [`create_index`](https://docs.pinecone.io/reference/create_index) function to create an index, name it `langchain-retrieval-augmentation-fast`, set the dimension to **1536**, and use the metric `cosine`.\n",
    "\n",
    "</div>\n",
    "\n",
    "\n",
    "**Note:** We set the dimension to 1536 as this is the dimension of OpenAI's text embedding model 'text-embedding-ada-002' that we use to embed the data. If you wish to use another model (for instance a Hugging Face model using HuggingFaceInferenceAPIEmbeddings), you will have to change this number to match the dimension of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   Check that your pinecone database does not contain any indexes, and delete them if there are any.\n",
    "#\n",
    "###########################\n",
    "#pinecone.delete_index(name = index_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "IndexDescription(name='langchain-retrieval-augmentation-fast', metric='cosine', replicas=1, dimension=1536.0, shards=1, pods=1, pod_type='starter', status={'ready': True, 'state': 'Ready'}, metadata_config=None, source_collection='')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "index_name = 'langchain-retrieval-augmentation-fast'\n",
    "\n",
    "# TODO : your code bellow\n",
    "try:\n",
    "    pinecone.describe_index(name=index_name)\n",
    "except:\n",
    "    pinecone.create_index(name=index_name,dimension = 1536, metric='cosine')\n",
    "\n",
    "pinecone.list_indexes()\n",
    "\n",
    "pinecone.describe_index(name=index_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Run the cell bellow to push the data on to the Vector Database. This can take a few minutes.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "ename": "PineconeException",
     "evalue": "UNKNOWN:DNS resolution failed for langchain-retrieval-augmentation-fast-too68lo.svc.gcp-starter.pinecone.io:443: UNAVAILABLE: WSA Error {grpc_status:14, created_time:\"2023-11-16T15:24:53.9107205+00:00\"}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_InactiveRpcError\u001b[0m                         Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Anaconda\\envs\\daps\\Lib\\site-packages\\pinecone\\core\\grpc\\index_grpc.py:184\u001b[0m, in \u001b[0;36mGRPCIndexBase._wrap_grpc_call.<locals>.wrapped\u001b[1;34m()\u001b[0m\n\u001b[0;32m    183\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m--> 184\u001b[0m     \u001b[39mreturn\u001b[39;00m func(request, timeout\u001b[39m=\u001b[39;49mtimeout, metadata\u001b[39m=\u001b[39;49m_metadata, credentials\u001b[39m=\u001b[39;49mcredentials,\n\u001b[0;32m    185\u001b[0m                 wait_for_ready\u001b[39m=\u001b[39;49mwait_for_ready, compression\u001b[39m=\u001b[39;49mcompression)\n\u001b[0;32m    186\u001b[0m \u001b[39mexcept\u001b[39;00m _InactiveRpcError \u001b[39mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\daps\\Lib\\site-packages\\grpc\\_channel.py:1161\u001b[0m, in \u001b[0;36m_UnaryUnaryMultiCallable.__call__\u001b[1;34m(self, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m   1155\u001b[0m (\n\u001b[0;32m   1156\u001b[0m     state,\n\u001b[0;32m   1157\u001b[0m     call,\n\u001b[0;32m   1158\u001b[0m ) \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_blocking(\n\u001b[0;32m   1159\u001b[0m     request, timeout, metadata, credentials, wait_for_ready, compression\n\u001b[0;32m   1160\u001b[0m )\n\u001b[1;32m-> 1161\u001b[0m \u001b[39mreturn\u001b[39;00m _end_unary_response_blocking(state, call, \u001b[39mFalse\u001b[39;49;00m, \u001b[39mNone\u001b[39;49;00m)\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\daps\\Lib\\site-packages\\grpc\\_channel.py:1004\u001b[0m, in \u001b[0;36m_end_unary_response_blocking\u001b[1;34m(state, call, with_call, deadline)\u001b[0m\n\u001b[0;32m   1003\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1004\u001b[0m     \u001b[39mraise\u001b[39;00m _InactiveRpcError(state)\n",
      "\u001b[1;31m_InactiveRpcError\u001b[0m: <_InactiveRpcError of RPC that terminated with:\n\tstatus = StatusCode.UNAVAILABLE\n\tdetails = \"DNS resolution failed for langchain-retrieval-augmentation-fast-too68lo.svc.gcp-starter.pinecone.io:443: UNAVAILABLE: WSA Error\"\n\tdebug_error_string = \"UNKNOWN:DNS resolution failed for langchain-retrieval-augmentation-fast-too68lo.svc.gcp-starter.pinecone.io:443: UNAVAILABLE: WSA Error {grpc_status:14, created_time:\"2023-11-16T15:24:53.9107205+00:00\"}\"\n>",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mPineconeException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\zjy\\github-classroom\\UCL-ELEC0136\\4-advanced-data-storage-LagrKevin6\\4-Advanced-Data-Storage.ipynb Cell 87\u001b[0m line \u001b[0;36m9\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zjy/github-classroom/UCL-ELEC0136/4-advanced-data-storage-LagrKevin6/4-Advanced-Data-Storage.ipynb#Y150sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m# wait a moment for the index to be fully initialized\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/zjy/github-classroom/UCL-ELEC0136/4-advanced-data-storage-LagrKevin6/4-Advanced-Data-Storage.ipynb#Y150sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m time\u001b[39m.\u001b[39msleep(\u001b[39m1\u001b[39m)\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/zjy/github-classroom/UCL-ELEC0136/4-advanced-data-storage-LagrKevin6/4-Advanced-Data-Storage.ipynb#Y150sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m index_me\u001b[39m.\u001b[39;49mdescribe_index_stats()\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zjy/github-classroom/UCL-ELEC0136/4-advanced-data-storage-LagrKevin6/4-Advanced-Data-Storage.ipynb#Y150sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m \u001b[39mfor\u001b[39;00m batch \u001b[39min\u001b[39;00m dataset\u001b[39m.\u001b[39miter_documents(batch_size\u001b[39m=\u001b[39m\u001b[39m100\u001b[39m):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/zjy/github-classroom/UCL-ELEC0136/4-advanced-data-storage-LagrKevin6/4-Advanced-Data-Storage.ipynb#Y150sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     index_me\u001b[39m.\u001b[39mupsert(batch)\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\daps\\Lib\\site-packages\\pinecone\\core\\grpc\\index_grpc.py:747\u001b[0m, in \u001b[0;36mGRPCIndex.describe_index_stats\u001b[1;34m(self, filter, **kwargs)\u001b[0m\n\u001b[0;32m    744\u001b[0m timeout \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\u001b[39m'\u001b[39m\u001b[39mtimeout\u001b[39m\u001b[39m'\u001b[39m, \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    746\u001b[0m request \u001b[39m=\u001b[39m DescribeIndexStatsRequest(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39margs_dict)\n\u001b[1;32m--> 747\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_wrap_grpc_call(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mstub\u001b[39m.\u001b[39;49mDescribeIndexStats, request, timeout\u001b[39m=\u001b[39;49mtimeout)\n\u001b[0;32m    748\u001b[0m json_response \u001b[39m=\u001b[39m json_format\u001b[39m.\u001b[39mMessageToDict(response)\n\u001b[0;32m    749\u001b[0m \u001b[39mreturn\u001b[39;00m parse_stats_response(json_response)\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\daps\\Lib\\site-packages\\pinecone\\core\\grpc\\index_grpc.py:189\u001b[0m, in \u001b[0;36mGRPCIndexBase._wrap_grpc_call\u001b[1;34m(self, func, request, timeout, metadata, credentials, wait_for_ready, compression)\u001b[0m\n\u001b[0;32m    186\u001b[0m     \u001b[39mexcept\u001b[39;00m _InactiveRpcError \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m    187\u001b[0m         \u001b[39mraise\u001b[39;00m PineconeException(e\u001b[39m.\u001b[39m_state\u001b[39m.\u001b[39mdebug_error_string) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n\u001b[1;32m--> 189\u001b[0m \u001b[39mreturn\u001b[39;00m wrapped()\n",
      "File \u001b[1;32mc:\\Anaconda\\envs\\daps\\Lib\\site-packages\\pinecone\\core\\grpc\\index_grpc.py:187\u001b[0m, in \u001b[0;36mGRPCIndexBase._wrap_grpc_call.<locals>.wrapped\u001b[1;34m()\u001b[0m\n\u001b[0;32m    184\u001b[0m     \u001b[39mreturn\u001b[39;00m func(request, timeout\u001b[39m=\u001b[39mtimeout, metadata\u001b[39m=\u001b[39m_metadata, credentials\u001b[39m=\u001b[39mcredentials,\n\u001b[0;32m    185\u001b[0m                 wait_for_ready\u001b[39m=\u001b[39mwait_for_ready, compression\u001b[39m=\u001b[39mcompression)\n\u001b[0;32m    186\u001b[0m \u001b[39mexcept\u001b[39;00m _InactiveRpcError \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m--> 187\u001b[0m     \u001b[39mraise\u001b[39;00m PineconeException(e\u001b[39m.\u001b[39m_state\u001b[39m.\u001b[39mdebug_error_string) \u001b[39mfrom\u001b[39;00m \u001b[39me\u001b[39;00m\n",
      "\u001b[1;31mPineconeException\u001b[0m: UNKNOWN:DNS resolution failed for langchain-retrieval-augmentation-fast-too68lo.svc.gcp-starter.pinecone.io:443: UNAVAILABLE: WSA Error {grpc_status:14, created_time:\"2023-11-16T15:24:53.9107205+00:00\"}"
     ]
    }
   ],
   "source": [
    "# Run this cell - It may take a few minutes\n",
    "\n",
    "import time\n",
    "\n",
    "index_me = pinecone.GRPCIndex(index_name) # GRPC allows for faster upserts to the Pinecone Vector Database\n",
    "# wait a moment for the index to be fully initialized\n",
    "time.sleep(1)\n",
    "\n",
    "index_me.describe_index_stats()\n",
    "\n",
    "for batch in dataset.iter_documents(batch_size=100):\n",
    "    index_me.upsert(batch)\n",
    "\n",
    "\n",
    "index_me.describe_index_stats()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Creating a vector store\n",
    "\n",
    "Now that we've build our index we can switch over to LangChain. We need to initialize a LangChain vector store using the same index we just built. For this we will also need a LangChain embedding object."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Run the following cells to initialize a LangChain vector store.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "\n",
    "model_name = 'text-embedding-ada-002'\n",
    "\n",
    "embed = OpenAIEmbeddings(\n",
    "    model=model_name,\n",
    "    openai_api_key=OPENAI_API_KEY\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "\n",
    "from langchain.vectorstores import Pinecone\n",
    "\n",
    "text_field = \"text\"\n",
    "\n",
    "# switch back to normal index for langchain\n",
    "index_me = pinecone.Index(index_name)\n",
    "\n",
    "vectorstore = Pinecone(\n",
    "    index_me, embed, text_field\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Run the following cell to perform a `similarity_search` of the content of the query on our [vectorstore](https://python.langchain.com/docs/modules/data_connection/vectorstores/) containing embedded information about our augmentation dataset.\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell\n",
    "\n",
    "query = \"When was Napoleon born\"\n",
    "\n",
    "vectorstore.similarity_search(\n",
    "    query,  # our search query\n",
    "    k=3  # return 3 most relevant docs\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Augmented LLM query\n",
    "\n",
    "We finally have all the pieces of the puzzle, and we can now force the LLM to answer a question based on the information it is seeing being returned from the vectorstore. This allows for many things, like giving it up-to-date information, but can also be used to [cite sources](https://python.langchain.com/docs/use_cases/question_answering/vector_db_qa#return-source-documents). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Run the following cell to create a `VectorStoreRetrieverMemory` object that we will pass on to the LLMChain.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "from langchain.memory.vectorstore import VectorStoreRetrieverMemory\n",
    "\n",
    "retriever = vectorstore.as_retriever(search_kwargs=dict(k=1))\n",
    "memory_RAG = VectorStoreRetrieverMemory(retriever=retriever)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "- Create a new prompt template with a question of your choice. For example, you can ask about the life of historical figures.\n",
    "\n",
    "- Using langchain.LLMChain, create a chain to run the new prompt template by our OpenAI LLM davinci. Add to the lists of arguments `memory = memory_RAG` to instruct the LLM to look for the answer in the vectorstore.\n",
    "\n",
    "\n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Task: \n",
    "#   Asks the language model to give the date of birth of a historical figure.\n",
    "#\n",
    "###########################\n",
    "\n",
    "# TODO : your code bellow\n",
    "prompt_me = PromptTemplate.from_template(\"Please find the Date of Birth of {name}\") #same as 2.1.2\n",
    "\n",
    "llm_Ch2 = LLMChain(prompt=prompt_me,llm = davinci,memory = memory_RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Augmented LLM query with sources\n",
    "\n",
    "As stated before, we can use retrieval augmentation to provide sources to the output of the LLM (provided the sources were in the metadata of the embedded dataset)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': 'Tell me about the life of Napoleon',\n",
       " 'answer': \" I don't know.\\n\",\n",
       " 'sources': ''}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Run this cell\n",
    "\n",
    "from langchain.chains import RetrievalQAWithSourcesChain\n",
    "\n",
    "chain = RetrievalQAWithSourcesChain.from_chain_type(llm = davinci, chain_type=\"stuff\", retriever=retriever)\n",
    "chain({\"question\": \"Tell me about the life of Napoleon\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>üë©‚Äçüíªüë®‚Äçüíª Optional action</b>\n",
    "\n",
    "- Reading: [Making Retrieval Augmented Generation Fast](https://www.pinecone.io/learn/fast-retrieval-augmented-generation/)\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. The end\n",
    "\n",
    "That's the end of this lab! We hope you learned a lot through it, and that you are now ready to go on the adventure on your own and explore all that is possible to do with LLMs and vector Databases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>üë©‚Äçüíªüë®‚Äçüíª Optional action</b>\n",
    "\n",
    "Explore the other exemple of applications of vector databases listed in Pinecone's documentation:\n",
    "\n",
    "https://docs.pinecone.io/page/examples\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To submit this assignment and **every other future assignment**, included the **final assignment** you have to:\n",
    "- Commit and push your code to GitHub\n",
    "- Go to **your** repository of the assignment. This must be on our course organisation `UCL-ELEC0136` and usually has the pattern `https://github.com/UCL-ELEC0136/<assignment-name>-<your-github-username>`.\n",
    "- Go in the `Pull requests` tab and click on the `Feedback` pull request.\n",
    "- Click on `Files changed` and verify that the files you have changed are listed.\n",
    "- Merge the pull request by clicking on `Merge pull request` and then `Confirm merge`.\n",
    "\n",
    "We are now ready to push our code that acquires data from GitHub to our repository (which is also GitHub, but this is just a coincidence, we could have used any other API, like Twitter's or Facebook's).\n",
    "\n",
    "<div class=\"alert alert-block alert-danger\">\n",
    "<b>üë©‚Äçüíªüë®‚Äçüíª Action required</b>\n",
    "\n",
    "Submit your assignment by following the steps above.\n",
    "</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "daps",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
